{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Translation using Transfer Model"
      ],
      "metadata": {
        "id": "qgLS-4ABOdpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip install torch\n",
        "!pip install sentencepiece\n",
        "!pip install transformers'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VmY6jeudHZHz",
        "outputId": "e50ed50e-46f2-4f88-8638-c5fea9f60ec0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip install torch\\n!pip install sentencepiece\\n!pip install transformers'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load the pre-trained English to Hindi translation model and tokenizer\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "2PabK4i_PkFe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for translation\n",
        "def translate_text(input_text):\n",
        "    # Tokenize the input text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate translation\n",
        "    output_ids = model.generate(input_ids, max_length=50, num_beams=5, length_penalty=0.6, no_repeat_ngram_size=2)\n",
        "\n",
        "    # Decode and return the translated text\n",
        "    translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return translated_text"
      ],
      "metadata": {
        "id": "KenLcSFsPzPv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example translation\n",
        "input_text = \"Hello, how are you?\"\n",
        "translated_text = translate_text(input_text)"
      ],
      "metadata": {
        "id": "7ay6FxnAP1IC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "print(\"Input Text: \", input_text)\n",
        "print(\"Translated Text: \", translated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0gdd3D7P3Dq",
        "outputId": "cec0953d-2164-4032-ff4b-3f32c6764814"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text:  Hello, how are you?\n",
            "Translated Text:  हैलो, तुम कैसे हो?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Reference translation\n",
        "reference_text = \"नमस्ते, आप कैसे हैं?\"\n",
        "\n",
        "# Translated text (your model's output)\n",
        "translated_text = \"हैलो, तुम कैसे हो?\"\n",
        "\n",
        "# Tokenize the reference and translated text\n",
        "reference_tokens = [token.lower() for token in reference_text.split()]\n",
        "translated_tokens = [token.lower() for token in translated_text.split()]\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = sentence_bleu([reference_tokens], translated_tokens)\n",
        "print(f'BLEU Score: {bleu_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS3jflGwQoop",
        "outputId": "22739239-168d-442e-c5af-615c9866e162"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 1.2882297539194154e-231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summerization using Transfer Model"
      ],
      "metadata": {
        "id": "JclDoK_ZOXyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load pre-trained T5 model and tokenizer for summarization\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXrNaPX8NeB0",
        "outputId": "e89ab4eb-1739-4133-ee50-0ce957293315"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample input text for summarization\n",
        "input_text = \"\"\"\n",
        "Generating random paragraphs can be an excellent way for writers to get their creative flow going at the beginning of the day. The writer has no idea what topic the random paragraph will be about when it appears. This forces the writer to use creativity to complete one of three common writing challenges. The writer can use the paragraph as the first one of a short story and build upon it. A second option is to use the random paragraph somewhere in a short story they create. The third option is to have the random paragraph be the ending paragraph in a short story. No matter which of these challenges is undertaken, the writer is forced to use creativity to incorporate the paragraph into their writing.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Eiu1y7bgNwNf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and generate summary\n",
        "inputs = tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)"
      ],
      "metadata": {
        "id": "jnUIOBVONya0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode and print the summary\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"Input Text:\\n\", input_text)\n",
        "print(\"\\nGenerated Summary:\\n\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiaGzI_VN0gT",
        "outputId": "c9fffb79-5ccd-4c44-8583-de113a6e3302"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text:\n",
            " \n",
            "Generating random paragraphs can be an excellent way for writers to get their creative flow going at the beginning of the day. The writer has no idea what topic the random paragraph will be about when it appears. This forces the writer to use creativity to complete one of three common writing challenges. The writer can use the paragraph as the first one of a short story and build upon it. A second option is to use the random paragraph somewhere in a short story they create. The third option is to have the random paragraph be the ending paragraph in a short story. No matter which of these challenges is undertaken, the writer is forced to use creativity to incorporate the paragraph into their writing.\n",
            "\n",
            "\n",
            "Generated Summary:\n",
            " writer has no idea what topic the random paragraph will be about when it appears. writer can use the paragraph as the first one of a short story and build upon it. writer can also use the random paragraph somewhere in a short story they create.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Comparison with other models\n",
        " Transformers are like smart tools for understanding and working with language. They are really good at handling different words and figuring out their relationships, even if they are far apart. Transformers learn from lots of examples before they are asked to do a specific job, and then they fine-tune themselves to do that job really well. They are better at understanding language than other tools like RNNs or LSTMs. Transformers are especially great for tasks like translating languages, where knowing the context of the words is crucial. However, using transformers can be like having a super powerful computer – it needs a lot of energy. So, for simpler tasks or when there isn't a lot of information, using transformers might be too much. In the end, choosing the right tool depends on what job needs to be done, what resources are available, and how much information there is, and transformers work really well in many language-related jobs."
      ],
      "metadata": {
        "id": "C-1Ur6T5JN-p"
      }
    }
  ]
}