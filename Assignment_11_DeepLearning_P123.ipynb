{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Literature Review\n",
        "\n",
        "The readings talk about a new kind of model called the Transformer. This model is a big deal because it does things differently from the usual models we use for tasks like translating languages. The old models, especially RNN, had a problem—they could only do one thing at a time, which made them slow and not very good for long sequences of data.\n",
        "\n",
        "The evolution from Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) models to Transformer architectures represents a paradigm shift in natural language processing. RNNs and LSTMs were early attempts at capturing sequential dependencies in data, making them suitable for tasks such as language modeling and machine translation. However, they faced challenges in handling long-range dependencies due to vanishing and exploding gradient problems.\n",
        "\n",
        "The Transformer architecture, introduced by Vaswani et al. in 2017, addressed these limitations by employing a self-attention mechanism, allowing the model to weigh different parts of the input sequence differently. This mechanism enables the Transformer to capture long-range dependencies more effectively, making it particularly well-suited for sequence-to-sequence tasks. The use of self-attention also facilitates parallelization, enhancing training efficiency.\n",
        "\n",
        "The Transformer fixes this issue by using something called attention instead of the old sequential way of doing things. This attention thing helps the model understand the relationships between different parts of the input and output all at once, without going through them one by one. This makes the Transformer much faster and more efficient during training.\n",
        "\n",
        "The Transformer has these special attention mechanisms that let it focus on different parts of the input and output as needed. This happens all at the same time, making things quicker. The model is made up of layers that use self-attention and fully connected parts, making it better at handling lots of data and giving top-notch results, especially in tasks like language translation. The Transformer model is a game-changer. It gets rid of the slow, one-at-a-time way of working that old models had by using attention, making it super efficient and really good at tasks that involve sequences of data."
      ],
      "metadata": {
        "id": "RPksc99k8yKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implementation of a Simple RNN (20% of the grade)\n",
        "\n",
        "• Implement a basic RNN model in Python using a framework like TensorFlow or PyTorch.\n",
        "\n",
        "• Use a small dataset (e.g., a subset of the IMDB movie reviews) for text classification or a\n",
        "similar task.\n",
        "\n",
        "• Analyze the performance and limitations of your RNN model."
      ],
      "metadata": {
        "id": "nrx8yLR1BI-m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrrQvTg1l3Lb",
        "outputId": "c0d646ea-8dc6-46f0-f739-53733cce28d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "235/235 [==============================] - 19s 63ms/step - loss: 0.6846 - accuracy: 0.5467 - val_loss: 0.6668 - val_accuracy: 0.5878\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 14s 61ms/step - loss: 0.5300 - accuracy: 0.7507 - val_loss: 0.6560 - val_accuracy: 0.6245\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 13s 57ms/step - loss: 0.3103 - accuracy: 0.8765 - val_loss: 0.5437 - val_accuracy: 0.7593\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 13s 56ms/step - loss: 0.1089 - accuracy: 0.9664 - val_loss: 0.8897 - val_accuracy: 0.6350\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 12s 52ms/step - loss: 0.0364 - accuracy: 0.9940 - val_loss: 0.7680 - val_accuracy: 0.7430\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.7540 - accuracy: 0.7431\n",
            "Test loss: 0.7540, Test accuracy: 0.7431\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load IMDB dataset\n",
        "max_features = 5000  # Number of words to consider as features\n",
        "max_len = 200  # Cuts off texts after this number of words\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to have a consistent length for RNN input\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Split the training set into 60% and 40%\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.4, random_state=42)\n",
        "\n",
        "# Define RNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(max_features, 32, input_length=max_len),\n",
        "    tf.keras.layers.SimpleRNN(32),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implementing an LSTM Model (20% of the grade)\n",
        "\n",
        "• Modify your RNN model to use LSTM units.\n",
        "\n",
        "• Compare its performance with the basic RNN model on the same task, highlighting the\n",
        "improvements or changes."
      ],
      "metadata": {
        "id": "0xPTCxd3BWtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM model\n",
        "model_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(max_features, 32, input_length=max_len),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the LSTM model\n",
        "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM model\n",
        "model_lstm.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the LSTM model on the test set\n",
        "loss_lstm, accuracy_lstm = model_lstm.evaluate(x_test, y_test)\n",
        "print(f'LSTM Test loss: {loss_lstm:.4f}, Test accuracy: {accuracy_lstm:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvJgwaMimaZg",
        "outputId": "92d80008-4a87-4c03-a00a-8db53cdf4930"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "235/235 [==============================] - 33s 124ms/step - loss: 0.4881 - accuracy: 0.7598 - val_loss: 0.3364 - val_accuracy: 0.8604\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 26s 110ms/step - loss: 0.2774 - accuracy: 0.8889 - val_loss: 0.3170 - val_accuracy: 0.8698\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 28s 120ms/step - loss: 0.2122 - accuracy: 0.9203 - val_loss: 0.3226 - val_accuracy: 0.8637\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 29s 122ms/step - loss: 0.1770 - accuracy: 0.9350 - val_loss: 0.3408 - val_accuracy: 0.8573\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 27s 115ms/step - loss: 0.1454 - accuracy: 0.9477 - val_loss: 0.4063 - val_accuracy: 0.8570\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.4197 - accuracy: 0.8529\n",
            "LSTM Test loss: 0.4197, Test accuracy: 0.8529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Exploring Attention Mechanisms (20% of the grade)\n",
        "\n",
        "• Implement a basic attention mechanism in your LSTM model.\n",
        "\n",
        "• Discuss how the attention mechanism impacts the model's performance and its ability to\n",
        "handle long-range dependencies."
      ],
      "metadata": {
        "id": "Jn7kPVYqBobN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WPS3WcsnyEG",
        "outputId": "d5a59695-1a51-4968-89b7-c46babb1fc84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.3/612.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.22.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        self.W_q = self.add_weight(name=\"W_q\",\n",
        "                                  shape=(input_shape[-1], input_shape[-1]),\n",
        "                                  initializer=\"uniform\",\n",
        "                                  trainable=True)\n",
        "        self.W_k = self.add_weight(name=\"W_k\",\n",
        "                                  shape=(input_shape[-1], input_shape[-1]),\n",
        "                                  initializer=\"uniform\",\n",
        "                                  trainable=True)\n",
        "        super(Attention, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, x):\n",
        "        q = tf.matmul(x, self.W_q)\n",
        "        k = tf.matmul(x, self.W_k)\n",
        "        v = x\n",
        "\n",
        "        attn_score = tf.matmul(q, k, transpose_b=True)\n",
        "        attn_score = tf.nn.softmax(attn_score, axis=-1)\n",
        "\n",
        "        output = tf.matmul(attn_score, v)\n",
        "        return output\n",
        "\n",
        "# Define LSTM with attention model\n",
        "model_lstm_attention = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(max_features, 32, input_length=max_len),\n",
        "    tf.keras.layers.LSTM(32, return_sequences=True),  # Return sequences for attention\n",
        "    Attention(),\n",
        "    tf.keras.layers.Flatten(),  # Flatten the output for the Dense layer\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the LSTM with attention model\n",
        "model_lstm_attention.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM with attention model\n",
        "model_lstm_attention.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the LSTM with attention model on the test set\n",
        "loss_lstm_attention, accuracy_lstm_attention = model_lstm_attention.evaluate(x_test, y_test)\n",
        "print(f'LSTM with Attention Test loss: {loss_lstm_attention:.4f}, Test accuracy: {accuracy_lstm_attention:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmxgbm8opyRC",
        "outputId": "c23c44e3-2ca5-4ed2-83aa-739426e27e9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "235/235 [==============================] - 57s 228ms/step - loss: 0.4244 - accuracy: 0.7913 - val_loss: 0.3251 - val_accuracy: 0.8637\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 46s 194ms/step - loss: 0.2583 - accuracy: 0.8959 - val_loss: 0.3188 - val_accuracy: 0.8644\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 42s 180ms/step - loss: 0.2081 - accuracy: 0.9173 - val_loss: 0.3417 - val_accuracy: 0.8613\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 42s 181ms/step - loss: 0.1804 - accuracy: 0.9292 - val_loss: 0.3722 - val_accuracy: 0.8529\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 42s 181ms/step - loss: 0.1526 - accuracy: 0.9402 - val_loss: 0.3881 - val_accuracy: 0.8546\n",
            "782/782 [==============================] - 27s 34ms/step - loss: 0.4000 - accuracy: 0.8469\n",
            "LSTM with Attention Test loss: 0.4000, Test accuracy: 0.8469\n"
          ]
        }
      ]
    }
  ]
}